{"00-Course-Overview/Course-Objectives":{"slug":"00-Course-Overview/Course-Objectives","filePath":"00 Course Overview/Course Objectives.md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"Course Objectives: Image Processing and Analysis\nThis note outlines the primary learning goals and required prerequisites for the course on Image Processing and Analysis.\nPrimary Goal\nThe main objective of this course is to provide students with the foundational knowledge required to master the end-to-end pipeline of digital image processing and analysis. Students will learn to apply specific techniques and algorithms relevant to a broad range of applications in computer vision and artificial intelligence.\nUpon completion, students will be able to:\n\nUnderstand the theoretical principles of digital image representation and manipulation.\nImplement algorithms for image preprocessing, enhancement, and noise reduction.\nAnalyze images to segment regions of interest and extract meaningful features.\nEvaluate the performance and results of different image processing techniques.\nConnect theoretical concepts to practical applications in various fields.\n\nInsight: The “Processing Chain”\nThe course emphasizes mastering the entire “chaîne de traitement” (processing chain). This is a crucial concept in applied computer vision. It refers to the logical sequence of operations—from initial image acquisition and cleanup (preprocessing) to high-level interpretation (feature extraction and decision-making)—required to solve a real-world problem. Understanding this pipeline is more valuable than knowing isolated algorithms.\nPrerequisites\nA solid foundation in the following areas is recommended for success in this course:\n\nAlgorithmics and Data Structures: Essential for understanding and implementing image processing operations efficiently.\nObject-Oriented Programming (OOP): The practical labs and implementations will rely on programming paradigms often structured within classes and objects.\nFoundational Mathematics: A baseline understanding of concepts from linear algebra (vectors, matrices), calculus, and probability is assumed from prior undergraduate studies.\n"},"00-Course-Overview/Course-Plan":{"slug":"00-Course-Overview/Course-Plan","filePath":"00 Course Overview/Course Plan.md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"Course Plan\nThis document outlines the sequential structure of the course, organized into five main chapters. Each chapter builds upon the previous one, following a logical progression from fundamental concepts to advanced analysis techniques.\nCourse Modules\n\n\nChapter 1: Definitions and Basic Concepts of Imaging\n\nThis foundational module introduces the digital image, defining key concepts such as pixels, resolution, color spaces, and histograms. It establishes the basic vocabulary and mathematical framework for the rest of the course.\n\n\n\nChapter 2: Image Preprocessing\n\nThis module focuses on techniques used to enhance image quality and prepare images for further analysis. Topics include noise filtering, contrast enhancement, and correcting degradations. Both spatial and frequency domain methods are covered.\n\n\n\nChapter 3: Image Segmentation\n\nSegmentation is the process of partitioning an image into multiple segments or regions of interest. This module will cover the algorithms used to isolate objects and boundaries within an image.\n\n\n\nChapter 4: Extraction of Characteristic Information\n\nOnce regions of interest are identified, this module explores methods for extracting meaningful data (features) from them. This includes describing shapes, colors, and textures in a quantitative way.\n\n\n\nChapter 5: Basic Concepts of Similarity and Image Matching\n\nThe final module introduces techniques for comparing images or features. This is fundamental to applications like object recognition, image retrieval, and tracking.\n\n\n"},"01-Introduction-to-Digital-Images/1.0-What-is-a-Digital-Image":{"slug":"01-Introduction-to-Digital-Images/1.0-What-is-a-Digital-Image","filePath":"01 Introduction to Digital Images/1.0 What is a Digital Image.md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"What is a Digital Image?\nThis note defines the concept of a digital image,  components, and its representation for computer processing.\nDefinition\nA digital image is a representation of a two-dimensional visual scene that has been acquired, processed, and stored in a numerical format. It is composed of a finite number of discrete elements, called pixels, arranged in a rectangular grid or matrix.\nUnlike an analog photograph, which has continuous variations in tone, a digital image is inherently discrete. Both its spatial coordinates (x,y) and its intensity or color values are quantized into finite numbers. This numerical representation allows a computer to store, process, and display the image.\nThe Pixel\nThe term pixel (short for “picture element”) is the smallest individual component of a digital image. Each pixel has two primary attributes:\n\nPosition: Its location within the image grid, defined by a pair of coordinates (e.g., row and column).\nValue: A numerical value that represents its intensity (for grayscale images) or color (for color images).\n\nAn image can be thought of as a function f(x,y) where x and y are spatial coordinates, and the value of f at any point (x,y) is the pixel’s intensity or color.\nMatrix Representation\nFrom a computer science perspective, a digital image is fundamentally a matrix of numbers.\n\nGrayscale Image: Represented as a 2D matrix where each element contains a single value corresponding to its brightness (e.g., 0 for black, 255 for white).\nColor Image: Typically represented as a 3D matrix (or three separate 2D matrices). Each pixel at (x,y) has a set of values, one for each color channel (e.g., Red, Green, and Blue).\n\nInsight: This matrix structure is the reason why libraries like NumPy in Python are so central to image processing. They provide highly optimized tools for performing mathematical operations on large arrays of numbers, which directly translates to manipulating images."},"01-Introduction-to-Digital-Images/1.1-Image-Representation-(Bitmap-vs-Vector)":{"slug":"01-Introduction-to-Digital-Images/1.1-Image-Representation-(Bitmap-vs-Vector)","filePath":"01 Introduction to Digital Images/1.1 Image Representation (Bitmap vs Vector).md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"Image Representation: Bitmap vs. Vector\nThis note clarifies the two primary methods for digitally representing images: bitmap (or raster) and vector.\nBitmap Images (Raster Graphics)\nBitmap images are the most common form of digital images and are the primary focus of this course.\nDefinition\nA bitmap image is composed of a grid of individual pixels. The image file contains the specific color or intensity value for every single pixel in the grid. This representation is analogous to a mosaic, where the overall picture is formed by a collection of small, colored tiles.\nCharacteristics\n\nResolution Dependent: The quality of a bitmap image is directly tied to its resolution (the number of pixels). Scaling a bitmap image to a larger size requires creating new pixels (interpolation), which often results in a loss of quality, leading to pixelation or blurriness.\nLarge File Sizes: Storing information for every pixel can lead to large files, especially for high-resolution images. Compression techniques (like JPEG) are often used to manage file size.\nIdeal for Photorealism: This pixel-by-pixel approach is perfect for representing complex, continuous-tone images like photographs and detailed digital paintings.\n\nExamples of Bitmap Formats: JPEG, PNG, GIF, BMP, TIFF.\nVector Images\nVector images use a different paradigm for representing visuals.\nDefinition\nA vector image is composed of mathematical objects such as points, lines, curves, and polygons. Instead of storing pixel values, the file stores a set of instructions that describe how to draw the image. For example, it might contain instructions like “draw a red circle with a radius of 20 pixels centered at coordinate (100, 150).”\nCharacteristics\n\nResolution Independent: Because vector images are based on mathematical formulas, they can be scaled to any size—from a business card to a billboard—without any loss of quality. The image is simply redrawn at the new size.\nSmall File Sizes: Storing mathematical instructions is generally more efficient than storing millions of pixel values.\nIdeal for Graphics: This format is best suited for illustrations, logos, icons, and typography, where clean lines and shapes are essential.\n\nExamples of Vector Formats: SVG, AI, EPS, PDF."},"01-Introduction-to-Digital-Images/1.2-Resolution-and-Color-Depth":{"slug":"01-Introduction-to-Digital-Images/1.2-Resolution-and-Color-Depth","filePath":"01 Introduction to Digital Images/1.2 Resolution and Color Depth.md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"Image Resolution and Color Depth\nThis note explains the two key variables that characterize a bitmap image: spatial resolution and color depth.\nSpatial Resolution\nDefinition\nSpatial resolution refers to the dimensions of an image, specifically the number of pixels it contains. It is typically expressed as the product of the image’s width and height in pixels (e.g., 1920 x 1080 pixels).\nA higher spatial resolution means the image is composed of more pixels. This allows for greater detail and clarity, as finer features can be represented. A lower resolution image has fewer pixels, resulting in a coarser, more “blocky” appearance.\nResolution for Display/Print (PPI/DPI)\nThe term “resolution” can also refer to the density of pixels within a physical area, measured in Pixels Per Inch (PPI) for digital displays or Dots Per Inch (DPI) for printing.\n\n1 inch = 2.54 centimeters\nThis metric determines how sharp an image appears when displayed or printed. A higher PPI means more pixels are packed into each inch, resulting in a sharper look.\n\nInsight: Spatial resolution (pixel dimensions) is an intrinsic property of the digital image file itself. PPI/DPI is metadata that suggests how large the image should be when rendered on a physical medium.\nColor Depth (Bit Depth)\nDefinition\nColor depth determines the number of distinct colors that can be represented for each pixel in an image. It is measured in bits per pixel (bpp). A higher color depth allows for a greater range of colors and smoother tonal transitions.\nCommon Color Depths\n\n1-bit (Binary Image): Each pixel can only be one of two colors (typically black or white). 2^1 = 2 values.\n8-bit (Grayscale): Each pixel has one of 256 possible shades of gray. 2^8 = 256 values (from 0 to 255).\n24-bit (True Color): This is the standard for most color images. It uses 8 bits for each of the three color channels (Red, Green, and Blue).\n\n8 bits (Red) + 8 bits (Green) + 8 bits (Blue) = 24 bits\nThis allows for 2^24 (approximately 16.7 million) distinct colors, which is generally more than the human eye can distinguish.\n\n\n"},"01-Introduction-to-Digital-Images/1.3-Color-Spaces-(RGB,-Grayscale,-HSL)":{"slug":"01-Introduction-to-Digital-Images/1.3-Color-Spaces-(RGB,-Grayscale,-HSL)","filePath":"01 Introduction to Digital Images/1.3 Color Spaces (RGB, Grayscale, HSL).md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"Color Spaces\nA color space, or color model, is an abstract mathematical model that describes how colors can be represented as tuples of numbers. It provides a structured way to define and manage colors in digital imaging.\nThe RGB Color Model\nThe RGB (Red, Green, Blue) model is an additive color model, meaning colors are created by mixing different intensities of red, green, and blue light. It is the most common color model for digital displays like monitors and cameras.\n\nRepresentation: A color is defined by a triplet of values (R, G, B), where each value represents the intensity of that primary color.\nOrigin: The point (0, 0, 0) corresponds to black (no light).\nWhite: The point (255, 255, 255) (in an 8-bit system) corresponds to white (full intensity of all three primaries).\nGrayscale: Values where R = G = B lie along the diagonal of the RGB color cube and represent shades of gray.\n\nGrayscale\nGrayscale is a color space that represents images using only shades of gray.\n\nRepresentation: Each pixel is represented by a single value indicating its luminance (intensity or brightness).\nConversion from RGB: A common method to convert an RGB image to grayscale is by calculating a weighted average of the R, G, and B components, as the human eye is not equally sensitive to all colors. A standard formula is:\nGrayscale = 0.299 * R + 0.587 * G + 0.114 * B\n\nOther Important Color Spaces\nWhile RGB is fundamental, other color spaces are designed to be more intuitive for human perception or better suited for specific processing tasks.\n\nHSL (Hue, Saturation, Lightness): Represents color in a way that is more aligned with human perception.\n\nHue: The type of color (e.g., red, yellow, green).\nSaturation: The “purity” or intensity of the color.\nLightness: The brightness of the color.\nInsight: This model is very useful for color manipulation tasks. For example, to change the color of an object, one can simply shift the Hue value while leaving Saturation and Lightness unchanged. This is much harder to do accurately in the RGB space.\n\n\nXYZ: A device-independent color space that serves as a standard reference.\nLab (CIELAB): Designed to be perceptually uniform, meaning a change of a certain amount in a color value should produce a change of about the same visual importance. It is excellent for measuring color differences.\nCMYK (Cyan, Magenta, Yellow, Key/Black): A subtractive color model used in printing.\n"},"01-Introduction-to-Digital-Images/1.4-Image-Formats-(JPG,-PNG,-DICOM)":{"slug":"01-Introduction-to-Digital-Images/1.4-Image-Formats-(JPG,-PNG,-DICOM)","filePath":"01 Introduction to Digital Images/1.4 Image Formats (JPG, PNG, DICOM).md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"Common Image Formats\nThis note describes several common image file formats and their primary use cases. The format determines how pixel data is stored, compressed, and what kind of metadata is included.\nCore Distinction: Lossy vs. Lossless Compression\n\nLossless Compression: Reduces file size without discarding any image data. The original image can be perfectly reconstructed from the compressed file. (e.g., PNG, TIFF).\nLossy Compression: Achieves much smaller file sizes by permanently discarding some image data. This data is chosen to be the least perceptible to the human eye. The original image cannot be perfectly recovered. (e.g., JPEG).\n\nBitmap Formats\n\n\nJPEG (Joint Photographic Experts Group):\n\nCompression: Lossy.\nUse Case: The most common format for photographs and other continuous-tone images on the web. It offers a great balance between image quality and file size. The compression level is adjustable, allowing a trade-off between quality and size.\nNot Ideal For: Images with sharp edges, text, or flat colors, as the compression can create visible artifacts.\n\n\n\nPNG (Portable Network Graphics):\n\nCompression: Lossless.\nUse Case: Excellent for web graphics, logos, and images that require transparency (alpha channel). It preserves sharp details, text, and flat colors perfectly.\nNot Ideal For: Photographs, as the lossless compression results in significantly larger file sizes compared to JPEG without a noticeable quality benefit for that type of image.\n\n\n\nBMP (Bitmap):\n\nCompression: Typically uncompressed.\nUse Case: A simple, raw format that stores pixel data directly. It is not efficient for storage or web use but can be useful for simple, programmatic image manipulation due to its straightforward structure.\n\n\n\nTIFF (Tagged Image File Format):\n\nCompression: Can be lossless (LZW, ZIP) or uncompressed. Can also support lossy JPEG compression.\nUse Case: A versatile and high-quality format often used in professional photography, desktop publishing, and document archiving. It supports high bit depths and multiple layers.\n\n\n\nRAW:\n\nDescription: Not a single format, but a category of formats that contain minimally processed data directly from a digital camera’s sensor.\nUse Case: Professional photography. It offers the maximum flexibility for post-processing adjustments (like white balance and exposure) as no data has been “baked in” by the camera’s processor.\n\n\n\nSpecialized Formats\n\nDICOM (Digital Imaging and Communications in Medicine):\n\nDescription: A standard format for storing and transmitting medical images (like MRIs, CT scans).\nInsight: DICOM is more than just an image format; it is a comprehensive protocol. A DICOM file contains not only the image data but also a rich set of metadata, including patient information, acquisition parameters, and device details, which is critical for clinical use.\n\n\n"},"01-Introduction-to-Digital-Images/1.5-The-Image-Histogram":{"slug":"01-Introduction-to-Digital-Images/1.5-The-Image-Histogram","filePath":"01 Introduction to Digital Images/1.5 The Image Histogram.md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"The Image Histogram\nThis note explains the concept of an image histogram, its construction, and its utility in image analysis and processing.\nDefinition\nAn image histogram is a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. In a simple sense, a histogram is a bar chart where the x-axis represents the range of possible pixel intensity values (e.g., 0 to 255 for an 8-bit grayscale image), and the y-axis represents the number of pixels in the image that have that specific intensity value.\nMathematical Formulation\nThe histogram H(x) of an image I can be defined as:\nH(x) = Card{P : I(P) = x}\nWhere:\n\nx is a specific intensity level.\nP represents a pixel in the image I.\nI(P) is the intensity value of pixel P.\nCard{} denotes the cardinality (the count) of the set of pixels that satisfy the condition I(P) = x.\n\nWhat a Histogram Reveals\nThe histogram is a powerful and simple tool that provides a summary of an image’s characteristics. By analyzing its shape, one can infer:\n\n\nBrightness:\n\nA histogram clustered to the left indicates a dark image (most pixels have low intensity values).\nA histogram clustered to the right indicates a bright image.\nA well-distributed histogram suggests a balanced image.\n\n\n\nContrast:\n\nA narrow histogram (occupying a small portion of the intensity range) indicates a low-contrast image. The tones are all very similar.\nA histogram that is spread out across the entire range of intensities indicates a high-contrast image.\n\n\n\nColor Histograms\nFor color images, histograms can be computed in two primary ways:\n\nCombined Histogram: First, convert the image to grayscale and compute a single histogram of the luminance values.\nPer-Channel Histograms: Compute three separate histograms, one for each color channel (Red, Green, and Blue). This provides a more detailed analysis of the color distribution.\n\nApplications\nHistograms are not just for analysis; they are a key component in many image processing techniques:\n\nContrast Enhancement (Histogram Equalization): A technique that redistributes pixel intensities to spread the histogram more evenly, thereby increasing global contrast.\nImage Segmentation (Thresholding): If an image contains a dark object on a light background, the histogram will often be bimodal (have two peaks). A threshold can be chosen in the valley between the peaks to separate the object from the background.\nImage Comparison: Histograms can be used as a simple feature to compare the similarity of two images.\n"},"01-Introduction-to-Digital-Images/1.6-Pixel-Relationships-and-Distance":{"slug":"01-Introduction-to-Digital-Images/1.6-Pixel-Relationships-and-Distance","filePath":"01 Introduction to Digital Images/1.6 Pixel Relationships and Distance.md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"This note explains how individual pixels relate to one another in terms of their spatial arrangement. Understanding these relationships—specifically adjacency, connectivity, and distance—is fundamental for nearly all image processing algorithms, from filtering to object detection.\nAdjacency and Neighbors\nAdjacency defines which pixels are considered “neighbors” to a central pixel. For a pixel P at coordinates (x, y), there are two primary definitions of its neighborhood.\n1. 4-Neighbors (N4)\nThe 4-neighbors are the pixels that are directly adjacent to pixel P horizontally and vertically.\n\nThe set of coordinates for N4(P) is:\n(x+1, y), (x-1, y), (x, y+1), (x, y-1)\nThis defines 4-connectivity. It corresponds to pixels that are a Manhattan distance of 1 from P.\n\n2. 8-Neighbors (N8)\nThe 8-neighbors include the 4-neighbors plus the four pixels that are diagonally adjacent to P.\n\nThe set of coordinates for N8(P) is:\n(x+1, y), (x-1, y), (x, y+1), (x, y-1),\n(x+1, y+1), (x+1, y-1), (x-1, y+1), (x-1, y-1)\nThis defines 8-connectivity. It corresponds to pixels that are a Chessboard distance of 1 from P.\n\nInsight: The choice between 4- and 8-connectivity is significant in practice. For example, when tracing the boundary of an object in a binary image, using 4-connectivity might perceive a thin diagonal line as disconnected, while 8-connectivity would see it as a continuous path.\nDistance Metrics Between Pixels\nA distance metric is a function that quantifies the spatial separation between two pixels, P(xp, yp) and Q(xq, yq).\n1. Euclidean Distance (d₂)\nThis is the standard, “straight-line” distance between two points. It is computed using the Pythagorean theorem.\n\nFormula: d₂(P,Q) = √((xp - xq)² + (yp - yq)²) \nUse Case: Provides a true geometric distance, but its calculation involves a square root, which can be computationally more expensive than other metrics.\n\n2. Manhattan Distance (d₁ or City-Block Distance)\nThis metric measures the distance by summing the absolute differences of the coordinates. It is constrained to horizontal and vertical movements only, like navigating a city grid.\n\nFormula: d₁(P,Q) = |xp - xq| + |yp - yq|\nUse Case: Computationally very fast. It defines a neighborhood of pixels that can be reached in a certain number of non-diagonal steps.\n\n3. Chebyshev Distance (d_inf or Chessboard Distance)\nThis metric is defined as the maximum of the absolute differences of the coordinates. It represents the minimum number of moves a king would need to travel between two squares on a chessboard.\n\nFormula: d_inf(P,Q) = max(|xp - xq|, |yp - yq|)\nUse Case: Useful for defining a square neighborhood where all pixels within the square are considered equidistant from the center.\n\nRelationship Between Metrics\nFor any two pixels P and Q, the distances are related by the following inequality:\nd_inf(P,Q) ≤ d₂(P,Q) ≤ d₁(P,Q)\nInsight: This relationship makes intuitive sense. The Chessboard distance is the “shortest” path because it allows diagonal shortcuts. The Manhattan distance is the “longest” because it’s restricted to grid lines. The Euclidean distance, the true straight line, lies between these two extremes. The choice of metric depends on the specific requirements of the algorithm, often balancing between geometric accuracy and computational efficiency."},"02-Image-Preprocessing-and-Filtering/2.0-Image-Noise-and-Degradation":{"slug":"02-Image-Preprocessing-and-Filtering/2.0-Image-Noise-and-Degradation","filePath":"02 Image Preprocessing and Filtering/2.0 Image Noise and Degradation.md","title":"Pixel Relationships and Distance","links":["In-Depth/Image-Quality-Metrics-(PSNR,-MSE)"],"tags":[],"content":"Image Noise and Degradation\nThis note defines image noise and degradation, outlines the most common noise models encountered in digital imaging, and introduces metrics for quantifying their impact.\nDefinition\nImage Degradation refers to any process that diminishes the quality of an image. This can include blurring, geometric distortions, poor contrast, or the introduction of noise.\nDigital Noise is a specific form of degradation characterized by unwanted, random variations in brightness or color information in an image. It arises during image acquisition (from the sensor), transmission, or processing, and it is not part of the original scene. Preprocessing aims to reduce or remove this noise.\nCommon Noise Models\nDifferent physical processes result in different statistical distributions of noise. Understanding these models is crucial for selecting the appropriate filtering technique.\n1. Additive Noise (Gaussian Noise)\nThis is the most common noise model. It assumes that the noise value B(i,j) is added to the pure image pixel value IP(i,j) to produce the corrupted image IB(i,j).\n\nFormula: IB(i,j) = IP(i,j) + B(i,j)\nCharacteristics: The noise values B are typically drawn from a Gaussian (normal) distribution with a mean of zero. The standard deviation (sigma) of the distribution controls the intensity of the noise.\nCause: Primarily caused by thermal noise in electronic sensors and amplifiers during image acquisition.\nAppearance: Fine, grain-like texture distributed across the entire image.\n\n2. Impulsive Noise (Salt-and-Pepper Noise)\nThis noise model affects only a random subset of pixels, replacing them with extreme values (either black or white).\n\nFormula:\n\nIB(i,j) = 0 (pepper) with probability p/2\nIB(i,j) = 255 (salt) with probability p/2\nIB(i,j) = IP(i,j) with probability 1-p\n\n\nCharacteristics: The noise manifests as sparse black and white dots scattered across the image.\nCause: Often due to sensor defects (dead or stuck pixels), transmission errors in a digital signal, or memory corruption.\nInsight: Because the noise corrupts pixels with extreme, non-representative values, filters based on averaging are ineffective. Rank-order filters, like the Median filter, are far more suitable.\n\n3. Multiplicative Noise (Speckle Noise)\nIn this model, the noise value is multiplied by the original pixel value.\n\nFormula: IB(i,j) = IP(i,j) * B(i,j)\nCharacteristics: The noise intensity is proportional to the local image intensity. Brighter regions of the image will appear noisier than darker regions.\nCause: Common in coherent imaging systems such as radar (SAR), sonar, and medical ultrasound.\n\nQuantifying Degradation\nTo objectively evaluate the effectiveness of a noise filter, we need metrics to measure the difference between the original (pure) image and the noisy or filtered image.\n\nMean Squared Error (MSE): Measures the average squared difference between pixel values.\nPeak Signal-to-Noise Ratio (PSNR): A logarithmic metric derived from the MSE. A higher PSNR generally indicates better image quality.\n\nThese metrics are critical for comparing the performance of different filtering algorithms, as covered in Image Quality Metrics (PSNR, MSE)."},"02-Image-Preprocessing-and-Filtering/2.1-Spatial-Domain-vs-Frequency-Domain":{"slug":"02-Image-Preprocessing-and-Filtering/2.1-Spatial-Domain-vs-Frequency-Domain","filePath":"02 Image Preprocessing and Filtering/2.1 Spatial Domain vs Frequency Domain.md","title":"Pixel Relationships and Distance","links":["In--Depth/Convolution","In--Depth/The-Fourier-Transform-in-Imaging"],"tags":[],"content":"This note explains the two fundamental domains in which image processing operations, particularly filtering, are performed.\nThe Spatial Domain\nThe spatial domain refers to the image plane itself, based on a Cartesian coordinate system (x,y). Operations in this domain work directly on the pixel values at these coordinates.\n\nRepresentation: An image is a matrix of intensity or color values.\nOperations: Spatial filtering involves a neighborhood operation, where the value of an output pixel is determined by the values of a small neighborhood of corresponding input pixels.\nMechanism: This is typically achieved using a filter mask (kernel) that slides across the image. The process is defined by Convolution.\nExamples of Filters: Mean filter, Gaussian filter, Median filter, Nagao filter.\nInsight: Spatial domain filtering is intuitive and computationally efficient for small kernels. Its operations are local, modifying a pixel based on its immediate surroundings.\n\nThe Frequency Domain\nThe frequency domain represents the image not by pixel location but by its frequency components. It describes the rate at which pixel values change across the image.\n\nRepresentation: The image is represented by its Fourier Transform, which decomposes the image into a sum of sine and cosine functions of different frequencies.\nOperations: Filtering is performed by modifying the coefficients of these frequencies in the Fourier spectrum.\nMechanism: The process involves transforming the image to the frequency domain, applying a filter mask via multiplication, and then transforming it back. This is detailed in The Fourier Transform in Imaging.\nExamples of Filters: Low-pass, high-pass, and band-pass filters.\nInsight: The frequency domain provides a global perspective on the image. It is particularly powerful for removing periodic noise and for implementing complex filters that would require very large kernels in the spatial domain. The core advantage stems from the property that computationally expensive convolution in the spatial domain becomes simple element-wise multiplication in the frequency domain.\n"},"02-Image-Preprocessing-and-Filtering/2.2-Linear-Spatial-Filters-(Mean,-Gaussian)":{"slug":"02-Image-Preprocessing-and-Filtering/2.2-Linear-Spatial-Filters-(Mean,-Gaussian)","filePath":"02 Image Preprocessing and Filtering/2.2 Linear Spatial Filters (Mean, Gaussian).md","title":"Pixel Relationships and Distance","links":["In-Depth/Convolution"],"tags":[],"content":"This note describes linear spatial filters, which operate on pixel neighborhoods and are implemented via convolution. Their primary use is for blurring and noise reduction.\nDefinition of a Linear Filter\nA spatial filter is considered linear if the value of the output pixel is a linear combination of the pixel values in the input neighborhood. This is mathematically equivalent to the Convolution operation, where the weights of the linear combination are the values in the filter’s kernel.\n1. Mean Filter (Averaging Filter)\nThe mean filter is the simplest linear filter. It replaces the value of each pixel with the unweighted average of the pixel values in a surrounding neighborhood.\n\nPrinciple: All pixels in the neighborhood contribute equally to the output.\nKernel: A matrix where all elements are 1/N, where N is the total number of elements in the kernel (e.g., 1/9 for a 3x3 kernel).\nEffect:\n\nReduces random (e.g., Gaussian) noise.\nCauses significant blurring of edges and fine details.\n\n\nDrawbacks: It is highly sensitive to outliers (like salt-and-pepper noise) and indiscriminately blurs all features, leading to a loss of image sharpness. A larger kernel size results in more aggressive blurring.\n\n2. Gaussian Filter\nThe Gaussian filter is a more sophisticated linear filter that also performs a weighted average, but the weights are determined by a Gaussian function.\n\nPrinciple: Pixels closer to the center of the kernel are given more weight than those farther away. The weights diminish smoothly with distance from the center.\nKernel: The kernel values are sampled from a 2D Gaussian distribution. The sigma (standard deviation) parameter of the distribution controls the extent of the blurring.\nEffect:\n\nEffective at reducing Gaussian noise.\nProduces a smoother, more natural-looking blur compared to the mean filter.\nProvides better preservation of edges than a mean filter of similar size.\n\n\nInsight: The sigma parameter provides fine-grained control over the smoothing effect. A small sigma produces a kernel that approximates the original image, while a large sigma results in heavy blurring. The Gaussian filter is also separable, meaning a 2D convolution can be optimized by performing two 1D convolutions, which is computationally much faster.\n"},"02-Image-Preprocessing-and-Filtering/2.3-Non-Linear-Spatial-Filters-(Median,-Nagao)":{"slug":"02-Image-Preprocessing-and-Filtering/2.3-Non-Linear-Spatial-Filters-(Median,-Nagao)","filePath":"02 Image Preprocessing and Filtering/2.3 Non-Linear Spatial Filters (Median, Nagao).md","title":"Pixel Relationships and Distance","links":[],"tags":[],"content":"This note covers non-linear spatial filters. Unlike linear filters, their output is not a weighted sum of input pixels. They are often more effective at specific tasks like removing impulsive noise or preserving edges during smoothing.\nDefinition of a Non-Linear Filter\nA non-linear filter operates on a pixel neighborhood, but the calculation used to determine the output pixel’s value is not a linear combination (i.e., it is not convolution). The operation is based on ordering or selecting pixel values within the neighborhood.\n1. Median Filter\nThe median filter is a powerful order-statistic filter, particularly effective against impulsive noise.\n\nPrinciple: It replaces the value of each pixel with the median value of the intensities in a surrounding neighborhood.\nProcess:\n\nConsider a neighborhood of pixels (e.g., a 3x3 window).\nCollect the intensity values of all pixels in this neighborhood.\nSort the values in ascending order.\nThe output value for the center pixel is the middle value from the sorted list.\n\n\nEffect:\n\nExcellent at removing salt-and-pepper noise.\nSignificantly better at preserving sharp edges compared to linear filters like the mean filter.\n\n\nInsight: The median filter’s strength comes from its robustness to outliers. An extreme value (like a black or white pixel from noise) will be sorted to either the beginning or end of the list and will therefore not be chosen as the median. This prevents the outlier from influencing the output, which is not the case in an averaging operation.\n\n2. Nagao Filter (Adaptive Filter)\nThe Nagao filter is an adaptive, edge-preserving smoothing filter. It modifies its behavior based on the local statistical properties of the image within a window.\n\nPrinciple: To calculate the new value for a center pixel, it first identifies the most uniform (least variance) sub-region within a larger neighborhood and then uses the average of only that sub-region.\nProcess:\n\nDefine a main window around the current pixel (e.g., 5x5).\nDivide this window into a set of smaller, overlapping sub-regions (e.g., nine 3x3 sub-regions).\nCalculate the variance of the pixel values within each sub-region.\nIdentify the sub-region with the minimum variance.\nThe output value is the mean of the pixel values in this minimum-variance sub-region.\n\n\nInsight: This adaptive process prevents blurring across edges. When the window is over a sharp edge, the sub-region with the lowest variance will be one that lies entirely on one side of the edge. By averaging only within this homogeneous region, the filter smooths noise without mixing pixel values from different sides of the contour, thus preserving the edge.\n"},"02-Image-Preprocessing-and-Filtering/2.4-Frequency-Domain-Filtering":{"slug":"02-Image-Preprocessing-and-Filtering/2.4-Frequency-Domain-Filtering","filePath":"02 Image Preprocessing and Filtering/2.4 Frequency Domain Filtering.md","title":"Pixel Relationships and Distance","links":["In-Depth/The-Fourier-Transform-in-Imaging","In-Depth/Gabor-Filters"],"tags":[],"content":"Core Concept\nInstead of operating on pixel neighborhoods, frequency domain filtering works by modifying the coefficients of the image’s constituent frequencies. The general workflow is:\n\nTransform the image into the frequency domain using the Fast Fourier Transform (FFT).\nMultiply the result by a filter mask designed to enhance or suppress certain frequencies.\nTransform the filtered result back into the spatial domain using the Inverse FFT.\n\nThe theoretical foundation is detailed in The Fourier Transform in Imaging.\nMain Filter Types\n1. Low-Pass Filter\nA low-pass filter preserves low-frequency components while attenuating or removing high-frequency components.\n\nMechanism: The filter mask has values of 1 at the center (where low frequencies are) and values of 0 towards the periphery.\nEffect:\n\nSmoothing and Blurring: Removing high frequencies eliminates sharp transitions, resulting in a smoother image.\nNoise Reduction: It is effective against random noise (like Gaussian noise), which typically consists of high-frequency components.\n\n\nAnalogy: This is the frequency-domain equivalent of spatial blurring filters like the mean or Gaussian filter.\n\n2. High-Pass Filter\nA high-pass filter preserves high-frequency components while attenuating low-frequency components.\n\nMechanism: The filter mask has values of 0 at the center and values of 1 towards the periphery.\nEffect:\n\nEdge Detection and Sharpening: By removing the smooth, low-frequency background, the filter enhances sharp details, edges, and fine textures.\n\n\nAnalogy: This is the frequency-domain equivalent of spatial edge detection kernels (e.g., Laplacian).\n\n3. Band-Reject Filter\nA band-reject filter removes a specific band of frequencies.\n\nMechanism: The filter mask is a ring that has values of 0 within a certain frequency range and 1 everywhere else.\nEffect: It is particularly useful for removing structured, periodic noise. Such noise appears as distinct bright spots in the frequency spectrum, and a band-reject filter can be precisely designed to eliminate them.\n\n4. Gabor Filter\nThe Gabor filter is a special linear filter used for texture analysis, edge detection, and feature extraction.\n\nPrinciple: It is a Gaussian kernel modulated by a sinusoidal plane wave.\nInsight: Its key property is that it is localized in both the spatial and frequency domains. This means it can be used to detect the presence of a specific frequency in a specific region of an image, making it ideal for analyzing textures that have a characteristic orientation and frequency. It is covered in more detail in Gabor Filters.\n"},"In-Depth/Convolution":{"slug":"In-Depth/Convolution","filePath":"In-Depth/Convolution.md","title":"Convolution","links":[],"tags":[],"content":"In-Depth: Image Convolution\nThis note provides a detailed explanation of image convolution, a fundamental operation in image processing used for applying filters to images.\nCore Concept\nImage convolution is an operation where each output pixel’s value is calculated as a weighted sum of the corresponding input pixel and its neighbors. The process involves sliding a small matrix, known as a kernel or filter, over the entire input image. At each position, an element-wise multiplication between the kernel and the overlapping image neighborhood is performed, and the results are summed to produce the value for the output pixel.\nThis “sliding window” mechanism allows the kernel to systematically modify the image based on local pixel information, making it the primary method for tasks like blurring, sharpening, edge detection, and noise reduction.\nThe Kernel (Filter/Mask)\nThe kernel is the small matrix of weights that defines the effect of the convolution. The values and size of the kernel determine the outcome of the filtering operation.\nKey Properties\n\nSize: Kernels typically have odd dimensions (e.g., 3x3, 5x5, 7x7). This ensures that the kernel has a clear center, which aligns with the current pixel being processed in the input image.\nSymmetry: Many common kernels are symmetric, meaning the weights are balanced around the center.\nSum of Elements: The sum of the kernel’s elements often has significance. For example, in blurring filters, the elements typically sum to 1 to preserve the overall brightness of the image.\n\nMathematical Definition\nThe convolution operation is denoted by the ⊗ symbol. For an input image f(x,y) and a filter h(x,y), the resulting image g(x,y) is:\ng(x,y) = f(x,y) ⊗ h(x,y)\nIn discrete terms, this is calculated using a double summation. For a pixel at (x,y), the value is computed as:\ng(x, y) = Σ Σ f(x + i, y + j) ⋅ h(i, j)\nWhere i and j are the indices iterating over the dimensions of the kernel h.\nThe Convolution Process: Step-by-Step\n\nSelect an Output Pixel: Choose a coordinate (x,y) in the output image to calculate.\nCenter the Kernel: Place the center of the kernel over the corresponding coordinate (x,y) in the input image.\nElement-Wise Multiplication: Multiply each element of the kernel by the value of the underlying pixel in the input image.\nSummation: Sum all the products from the previous step.\nAssign Value: The resulting sum is the new value for the output pixel at (x,y).\nRepeat: Repeat this process for every pixel in the image.\n\nProperties of Convolution\nConvolution has several mathematical properties that are important in image processing:\n\nCommutativity: f ⊗ h = h ⊗ f\n\nThe order of the image and the filter does not matter.\n\n\nAssociativity: (f ⊗ h1) ⊗ h2 = f ⊗ (h1 ⊗ h2)\n\nApplying two filters sequentially is the same as applying a single filter that is the convolution of the two individual filters.\n\n\nDistributivity: f ⊗ (h1 + h2) = (f ⊗ h1) + (f ⊗ h2)\n\nApplying a filter that is the sum of two other filters is the same as applying each filter individually and summing the results.\n\n\nSeparability: A 2D kernel h(x,y) is separable if it can be expressed as the product of two 1D vectors: h(x,y) = hx(x) * hy(y).\n\nInsight: This property is extremely valuable for optimization. Instead of a single 2D convolution, one can perform two much faster 1D convolutions (one along the rows, one along the columns). This significantly reduces computational complexity, especially for large kernels.\n\n\n\nHandling Image Borders (Edge Effects)\nWhen the kernel is positioned over pixels at the edge of the image, part of the kernel will extend beyond the image boundary. There are several common strategies to handle this:\n\nDo Not Filter Borders (Cropping): The simplest method is to not calculate new values for the border pixels. The output image will be slightly smaller than the input.\nPadding with Zeros: The input image is extended by adding a border of black pixels (value 0). This allows the kernel to operate on all original pixels but can introduce a dark edge artifact.\nMirror Padding (Reflection): The border is extended by reflecting the pixel values from within the image. This is often a more effective method as it maintains a more natural transition at the edges and reduces artifacts.\n"},"In-Depth/Gabor-Filters":{"slug":"In-Depth/Gabor-Filters","filePath":"In-Depth/Gabor Filters.md","title":"Gabor Filters","links":[],"tags":[],"content":"In-Depth: Principles of Gabor Filtering\nThis note covers the Gabor filter, a specialized linear filter widely used in computer vision for texture analysis, feature extraction, and edge detection.\nCore Concept\nA Gabor filter is a linear filter whose impulse response is defined by a sinusoidal function multiplied by a Gaussian function. This unique construction gives it properties of both the frequency and spatial domains.\nConstruction\nThe filter is essentially a Gaussian kernel that has been modulated by a plane wave (a sine or cosine wave).\n\nGaussian Component: This acts as an “envelope,” localizing the filter to a specific region of the image. It ensures that the analysis is focused on a neighborhood around the current pixel.\nSinusoidal Component: This is a wave with a specific frequency and orientation. It makes the filter sensitive to patterns that match this frequency and orientation.\n\nKey Parameters\nThe behavior of a Gabor filter is controlled by several parameters defined in its mathematical formula:\n\nθ (theta): The orientation of the sinusoidal wave. This determines which direction of edges or lines the filter is most sensitive to (e.g., 0° for vertical lines, 90° for horizontal lines).\nw or λ (lambda): The wavelength or frequency of the sinusoidal wave. This controls the thickness of the stripes the filter is tuned to detect.\nσ (sigma): The standard deviation of the Gaussian envelope. This determines the size of the receptive field (the neighborhood the filter considers).\n\nThe Power of Dual Localization\nThe primary strength of the Gabor filter is its optimal localization in both the spatial domain and the frequency domain.\n\nSpatial Localization: Due to the Gaussian envelope, the filter’s response is concentrated in a small area of the image.\nFrequency Localization: Due to the sinusoidal component, the filter is tuned to respond strongly to a narrow band of frequencies and orientations.\n\nInsight: This dual localization allows the Gabor filter to identify what kind of pattern exists (frequency/orientation) and where it exists in the image (spatial location). This makes it exceptionally powerful for texture classification, as a texture can be defined by the distribution of specific oriented patterns across an area.\nGabor Filter Banks\nIn practice, a single Gabor filter is not used in isolation. Instead, a filter bank is created, which consists of multiple Gabor filters with a range of different orientations and frequencies. By convolving an image with each filter in the bank, a rich set of feature responses is generated. This set of responses can then be used as a feature vector to train a machine learning classifier for tasks like texture recognition or material identification."},"In-Depth/Image-Quality-Metrics-(PSNR,-MSE)":{"slug":"In-Depth/Image-Quality-Metrics-(PSNR,-MSE)","filePath":"In-Depth/Image Quality Metrics (PSNR, MSE).md","title":"Image Quality Metrics (PSNR, MSE)","links":[],"tags":[],"content":"This note explains the purpose and calculation of Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR), two common metrics used to objectively measure the quality of a processed or compressed image against an original, pristine reference image.\nPurpose of Quality Metrics\nIn image processing, filtering operations aim to improve an image (e.g., by removing noise), but they can also introduce artifacts (e.g., blurring). Visual inspection is subjective. Objective metrics provide a quantitative, reproducible way to measure the fidelity of a result and compare the performance of different algorithms.\nMean Squared Error (MSE)\nMSE measures the cumulative squared error between two images. It represents the average of the squares of the differences between the pixel intensity values of the original and processed images.\nCalculation\nFor two grayscale images, Im (original) and Imf (filtered), of size H x W:\n          1      H-1  W-1\nMSE = ------- *  Σ    Σ   (Im[i,j] - Imf[i,j])²\n        H * W    i=0  j=0\n\nInterpretation\n\nA lower MSE value indicates a smaller difference between the images, meaning higher fidelity.\nAn MSE of 0 means the images are identical.\nInsight: The squaring of the difference term means that large errors are penalized much more heavily than small errors.\n\nPeak Signal-to-Noise Ratio (PSNR)\nPSNR is a more common metric that builds upon MSE. It measures the ratio between the maximum possible power of a signal (the image) and the power of the corrupting noise that affects its fidelity.\nCalculation\nPSNR is defined on a logarithmic scale (decibels, dB):\n             R²\nPSNR = 10 * log10(----)\n             MSE\n\nWhere:\n\nR is the maximum possible pixel value of the image. For an 8-bit grayscale image, R = 255.\nMSE is the Mean Squared Error calculated previously.\n\nInterpretation\n\nA higher PSNR value indicates a better quality reconstruction or a more effective filtering result.\nBecause of the logarithmic scale, PSNR aligns better with human perception of quality than MSE does.\nTypical values for image processing range from 20 dB (poor) to over 40 dB (excellent). An infinite PSNR occurs only if the MSE is 0 (identical images).\n"},"In-Depth/Mathematical-Morphology":{"slug":"In-Depth/Mathematical-Morphology","filePath":"In-Depth/Mathematical Morphology.md","title":"Mathematical Morphology","links":[],"tags":[],"content":"In-Depth: Foundations of Mathematical Morphology\nThis note details the principles of Mathematical Morphology, a theory and technique for the analysis and processing of geometrical structures, based on set theory. It is used for tasks like noise removal, object separation, and finding skeletons.\nCore Concept\nMorphological operations probe an image with a small shape or template known as a Structuring Element (SE). The SE is positioned at all possible locations in the image, and it is compared with the corresponding neighborhood of pixels. The operation’s outcome depends on whether the SE “fits” or “hits” the objects in the image. These operations are particularly useful for binary images.\nThe Structuring Element (SE)\nThe Structuring Element is a small binary matrix that defines the neighborhood and the shape of the probe.\n\nShape: It can be any shape, such as a square, a cross, or a disk. The shape of the SE determines what structures are sensitive to the operation.\nOrigin: The SE has an origin or center point, which is used to align it with the pixel being processed.\n\nFundamental Operations\nAll morphological operations are based on two primitive functions: Erosion and Dilation.\n1. Erosion\nErosion shrinks the boundaries of foreground objects (typically white pixels).\n\nPrinciple: The output pixel is set to the foreground value (1) only if the entire structuring element fits completely within the foreground region of the input image at that location. If any part of the SE overlaps the background, the output pixel is set to the background value (0).\nEffect:\n\nShrinks the size of objects.\nRemoves small, isolated noise particles (“islands”).\nCan cause objects to break apart.\n\n\n\n2. Dilation\nDilation expands the boundaries of foreground objects.\n\nPrinciple: The output pixel is set to the foreground value (1) if at least one pixel of the structuring element overlaps with a foreground pixel in the input image.\nEffect:\n\nIncreases the size of objects.\nFills small holes and gaps within objects.\nCan cause nearby objects to merge.\n\n\n\nCompound Operations\nErosion and Dilation are often used in sequence to achieve more complex filtering without globally shrinking or growing the objects.\n1. Opening\nOpening is defined as an erosion followed by a dilation, using the same structuring element.\n\nProcess: Opening(Image) = Dilate(Erode(Image))\nInsight: The initial erosion removes small noise particles and breaks thin connections. The subsequent dilation restores the size of the remaining large objects but does not bring back the small elements that were removed.\nEffect:\n\nSmooths object contours.\nRemoves small protrusions and isolated pixels.\nSeparates objects that are connected by a thin bridge.\n\n\n\n2. Closing\nClosing is defined as a dilation followed by an erosion, using the same structuring element.\n\nProcess: Closing(Image) = Erode(Dilate(Image))\nInsight: The initial dilation fills small holes and connects nearby objects. The subsequent erosion shrinks the objects back to their original size but retains the filled holes and connections.\nEffect:\n\nFills small holes within objects.\nConnects objects that are close together.\nSmooths contours by filling in small gaps.\n\n\n"},"In-Depth/The-Fourier-Transform-in-Imaging":{"slug":"In-Depth/The-Fourier-Transform-in-Imaging","filePath":"In-Depth/The Fourier Transform in Imaging.md","title":"The Fourier Transform in Imaging","links":[],"tags":[],"content":"From Spatial to Frequency Domain\nA digital image is typically represented in the spatial domain, where each point corresponds to a pixel value at a specific (x,y) coordinate. The Fourier Transform is a mathematical tool that converts this spatial representation into the frequency domain.\n\nSpatial Domain: Describes where things are in the image (pixel locations).\nFrequency Domain: Describes the rate of change of pixel values. It shows the constituent frequencies that make up the image.\n\nUnderstanding Image Frequencies\n\nLow Frequencies: Correspond to areas of the image where pixel values change slowly and smoothly. This represents the coarse structure and general shapes (e.g., a clear sky, a wall).\nHigh Frequencies: Correspond to areas where pixel values change rapidly. This represents fine details, edges, textures, and noise.\n\nIn a visual representation of the Fourier Transform (the magnitude spectrum), low frequencies are typically located at the center, and high frequencies are at the periphery.\nThe Key Insight: Convolution vs. Multiplication\nThe most important reason to use the Fourier Transform in image processing is a powerful mathematical property:\n\nConvolution in the spatial domain is equivalent to element-wise multiplication in the frequency domain.\n\nThis means that the computationally expensive process of sliding a kernel over an image can be replaced by a much faster process:\n\nTransform both the image and the filter’s kernel into the frequency domain using the Fast Fourier Transform (FFT).\nPerform a simple element-wise multiplication of the two resulting matrices.\nTransform the result back to the spatial domain using the Inverse FFT (IFFT).\n\nThe Filtering Process in the Frequency Domain\n\nForward FFT: Compute the FFT of the input image. This results in a complex-valued matrix.\nCreate a Frequency Domain Filter: Design a filter (mask) of the same size. This mask will have values of 1 for frequencies to be kept and 0 for frequencies to be removed.\n\nLow-Pass Filter: Keeps low frequencies (center of the spectrum) and removes high frequencies. The result is a blurring/smoothing effect.\nHigh-Pass Filter: Keeps high frequencies and removes low frequencies. This accentuates edges and details.\nBand-Pass/Reject Filter: Selectively keeps or removes a specific range of frequencies.\n\n\nApply the Filter: Multiply the FFT of the image by the filter mask element-wise.\nInverse FFT: Compute the IFFT of the resulting matrix to transform it back into the spatial domain, yielding the filtered image.\n\nImplementation Notes (Python/OpenCV)\nThe FFT is typically implemented using the numpy library in Python.\nimport numpy as np\nimport cv2\n \n# 1. Compute the FFT of a grayscale image\n# The result is a complex array.\nf = np.fft.fft2(input_image)\n \n# 2. Shift the zero-frequency component to the center for visualization and filtering\n# The raw FFT output has the low frequencies at the corners.\nfshift = np.fft.fftshift(f)\n \n# At this point, you would create and apply your filter mask to fshift.\n \n# 3. To transform back, first perform the inverse shift\nf_ishift = np.fft.ifftshift(fshift_after_filtering)\n \n# 4. Compute the inverse FFT\nimg_back = np.fft.ifft2(f_ishift)\nimg_back = np.abs(img_back) # Take the absolute value to get the real image"},"Labs/TP1---Setup-and-Basic-Image-Manipulation":{"slug":"Labs/TP1---Setup-and-Basic-Image-Manipulation","filePath":"Labs/TP1 - Setup and Basic Image Manipulation.md","title":"TP1 - Setup and Basic Image Manipulation","links":[],"tags":[],"content":"Lab 1: Environment Setup and Basic Image Manipulation\nObjective: To set up the Python development environment using PyCharm and to understand how digital images are represented and manipulated at the pixel level using the OpenCV and NumPy libraries.\n1. Environment and Project Setup (Exercise 1)\nThis initial exercise involves configuring the necessary tools for the course. The chosen tools provide a standard, powerful stack for computer vision tasks.\n\nPython: The programming language used for its simplicity, readability, and extensive ecosystem of libraries.\nPyCharm (Community Edition): An Integrated Development Environment (IDE) that provides code editing, debugging, and project management features, streamlining the development process.\nVirtual Environment: PyCharm creates a virtual environment for the project.\n\nInsight: A virtual environment is a critical best practice. It isolates the project’s dependencies (like specific versions of OpenCV and NumPy) from other Python projects on the system. This prevents version conflicts and ensures the project is reproducible on other machines.\n\n\nOpenCV-Python: A specialized library for computer vision. It provides a vast collection of functions for image/video analysis, processing, and machine learning.\nNumPy: A fundamental library for numerical computing in Python.\n\nInsight: OpenCV represents images as NumPy arrays. Therefore, proficiency in NumPy is essential for any efficient image manipulation in Python.\n\n\n\n2. Fundamental Image Operations (Exercise 2)\nThis section covers the core mechanics of loading, displaying, and accessing image data.\nLoading and Displaying an Image\nThe following functions are the primary tools for image I/O:\n\ncv2.imread(&#039;path/to/image&#039;, flag): Loads an image from a file. The flag determines the loading mode (e.g., cv2.IMREAD_COLOR, cv2.IMREAD_GRAYSCALE).\ncv2.imshow(&#039;Window Name&#039;, image_array): Displays an image in a window.\ncv2.waitKey(0): Pauses the program execution indefinitely until a key is pressed. This is necessary to keep the image window visible.\ncv2.destroyAllWindows(): Closes all open OpenCV windows.\n\nImage Representation in Memory\nThe most critical concept is that a digital image, when loaded by OpenCV, is nothing more than a NumPy array.\n\nGrayscale Image: A 2D array where each element is an integer (typically 0-255) representing the intensity of a single pixel.\nColor Image: A 3D array of shape (height, width, channels). Each element is a vector representing the color of a pixel.\n\nAccessing Image Properties\nThe shape attribute of the NumPy array provides the image’s dimensions.\n\nheight, width = gray_image.shape\nheight, width, channels = color_image.shape\n\nheight: The number of pixel rows.\nwidth: The number of pixel columns.\nchannels: The number of color components per pixel (e.g., 3 for BGR).\n\n\n\nAccessing and Modifying Pixel Values\nPixel values are accessed using NumPy’s array indexing.\n\nSyntax: image[row, column]\nCoordinates vs. Indexing: It is crucial to distinguish between (x, y) Cartesian coordinates and (row, col) NumPy indexing.\n\nx corresponds to column.\ny corresponds to row.\nTherefore, accessing a pixel at (x=150, y=100) is done with pixel = image[100, 150].\n\n\nColor Order: OpenCV loads color images in BGR (Blue, Green, Red) order by default, not the more common RGB.\n\npixel = color_image[100, 150] would return a list like [blue_value, green_value, red_value].\n\n\nModification: To change a pixel’s color, simply assign a new value:\ncolor_image[100, 150] = [255, 0, 0] (sets the pixel to pure blue).\n\n3. Fixed Thresholding (Exercise 3)\nThresholding is a foundational segmentation technique used to create a binary image from a grayscale image.\n\nConcept: A threshold value is chosen. All pixels with an intensity value above the threshold are set to a maximum value (e.g., 255 for white), and all pixels with an intensity value at or below the threshold are set to a minimum value (e.g., 0 for black).\nAlgorithm:\nfor each pixel in the grayscale image:\n    If pixel_value &gt; threshold:\n        set output_pixel_value = 255\n    Else:\n        set output_pixel_value = 0\n\nPurpose: This process is used to isolate objects of interest from the background, assuming they have a distinct intensity range. It is a simple yet effective way to preprocess an image for further analysis, like object counting or shape analysis.\n"},"Labs/TP2---Noise-Generation-and-Filtering":{"slug":"Labs/TP2---Noise-Generation-and-Filtering","filePath":"Labs/TP2 - Noise Generation and Filtering.md","title":"TP2 - Noise Generation and Filtering","links":["In-Depth/Image-Quality-Metrics-(PSNR,-MSE)"],"tags":[],"content":"1. Simulating Image Noise (Exercise 1)\nNoise is any unwanted random variation in image brightness or color information. This exercise simulates two of the most common types.\nAdditive Noise (Gaussian)\n\nConcept: Gaussian noise is characterized by adding a random value from a Gaussian (normal) distribution to each pixel’s intensity.\nCharacteristics:\n\nIt affects every pixel in the image.\nIt is described by its mean (μ) and variance (σ²). A higher variance results in more intense noise.\nIt is a good model for sensor noise caused by poor lighting or high temperature.\n\n\nSimulation: To add Gaussian noise, one generates a matrix of random numbers with a Gaussian distribution (mean 0, specified variance) and adds it to the original image matrix.\n\nImpulse Noise (Salt &amp; Pepper)\n\nConcept: Impulse noise is characterized by randomly replacing a fraction of image pixels with either a maximum value (salt/white) or a minimum value (pepper/black).\nCharacteristics:\n\nIt affects only a subset of pixels.\nIt is described by the density or percentage (p) of corrupted pixels.\nIt is a good model for errors in data transmission or faulty memory locations in hardware.\n\n\nSimulation: To add salt &amp; pepper noise, one randomly selects a percentage of pixel locations. For each selected location, it is randomly assigned a value of either 0 or 255.\n\n2. Applying Spatial Filters (Exercise 2)\nSpatial filtering involves applying an operation (via a kernel or neighborhood rule) to a group of pixels to calculate a new value for the central pixel.\nThe Mean Filter (Linear)\n\nMechanism: This is a linear filter that replaces the value of each pixel with the average (mean) of the intensity values in its neighborhood. This is achieved through convolution with a kernel where all elements are equal (e.g., 1/9 for a 3x3 kernel).\nEffectiveness:\n\nGood for Gaussian Noise: Since Gaussian noise is a zero-mean random process, averaging tends to cancel it out, resulting in a smoother image.\nPoor for Impulse Noise: An extreme outlier (like a white pixel in a dark area) will significantly skew the average, leaving a visible smudge instead of removing the noise.\n\n\nDrawback: It blurs the entire image, softening edges and fine details, which is often an undesirable side effect.\n\nThe Median Filter (Non-Linear)\n\nMechanism: This is a non-linear filter. For each pixel, it considers the intensity values in its neighborhood, sorts them in ascending order, and replaces the central pixel’s value with the median of that list.\nEffectiveness:\n\nExcellent for Impulse Noise: Extreme outliers (salt or pepper pixels) are moved to the beginning or end of the sorted list and are thus ignored by the median calculation. The filter effectively removes the noise without being influenced by the corrupted values.\nGood for Edge Preservation: Compared to the mean filter, the median filter is much better at preserving sharp edges, as the median is more robust to the variations that define an edge.\n\n\nInsight: The choice of filter is highly dependent on the type of noise. The Median filter is specifically designed to combat impulse noise, while the Mean filter is a general-purpose smoother best suited for additive, random noise like Gaussian.\n\n3. Evaluating Filter Performance\nTo objectively measure how well a filter works, we compare the filtered image to the original, noise-free image.\n\nMean Squared Error (MSE): Calculates the average squared difference between the pixel values of two images. A lower MSE indicates a better result.\nPeak Signal-to-Noise Ratio (PSNR): A logarithmic metric based on the MSE and the maximum possible pixel value (255).\n\nInsight: PSNR is measured in decibels (dB). A higher PSNR value indicates a better quality reconstruction, meaning the filtered image is closer to the original, noise-free image. It is the standard way to quantify the performance of noise reduction algorithms. You can learn more in the Image Quality Metrics (PSNR, MSE) note.\n\n\n"},"Labs/TP3---Nagao-Filter-and-Performance":{"slug":"Labs/TP3---Nagao-Filter-and-Performance","filePath":"Labs/TP3 - Nagao Filter and Performance.md","title":"TP3 - Nagao Filter and Performance","links":[],"tags":[],"content":"Lab 3: The Nagao Filter and Performance Analysis\nObjective: To implement and understand the Nagao filter, an advanced non-linear spatial filter designed for edge-preserving smoothing, and to evaluate its performance against simpler filters.\n1. The Concept of the Nagao Filter\nThe Nagao filter is an adaptive smoothing filter that aims to reduce noise without blurring important structural details like edges and corners. It achieves this by being selective about the neighborhood it uses for averaging.\nThe Limitation of Simpler Filters\n\nMean Filter: Averages indiscriminately, blurring everything.\nMedian Filter: Preserves edges better but can still degrade fine textures and corners.\n\nThe Nagao Mechanism\nThe filter operates on a local neighborhood (e.g., 5x5) around each pixel. Instead of using the entire neighborhood for calculation, it first divides the neighborhood into several smaller sub-regions.\n\nDefine Sub-Regions: Within the 5x5 window, the algorithm defines a set of pre-determined sub-regions (typically 9 regions of 7 pixels each, including the center). These sub-regions are shaped to represent different orientations (lines, corners).\nCalculate Variance: For each of these sub-regions, it calculates the intensity variance.\n\nVariance is a measure of how spread out the pixel values are. A low variance indicates a smooth, homogeneous region, while a high variance indicates a region with edges or texture.\n\n\nIdentify the Smoothest Sub-Region: The algorithm identifies the sub-region with the minimum variance. This is the most homogeneous (smoothest) sub-region in the neighborhood.\nCalculate the Mean: It then calculates the mean (average) intensity of the pixels only within that winning sub-region.\nAssign New Value: The central pixel of the 5x5 window is replaced with this calculated mean.\n\nThe Core Insight\nBy selecting the sub-region with the lowest variance, the Nagao filter intelligently adapts to the local image structure.\n\nIn a flat, uniform area: All sub-regions will have low variance, and the result will be similar to a standard mean filter.\nNear an edge: The sub-regions that lie along the edge will be relatively homogeneous and have low variance. The sub-regions that lie across the edge will contain a sharp transition and have high variance. The filter will choose a sub-region aligned with the edge, thus averaging pixels on the same side of the edge and preserving its sharpness.\n\n2. Implementation and Performance\nImplementation Details\nImplementing the Nagao filter requires a nested loop structure:\n\nIterate through each pixel of the input image (excluding borders).\nFor each pixel, extract the 5x5 neighborhood.\nWithin the neighborhood, iterate through the 9 pre-defined sub-regions.\nFor each sub-region, calculate its variance and mean.\nKeep track of the sub-region with the minimum variance found so far.\nAfter checking all sub-regions, assign the mean of the minimum-variance sub-region to the output pixel.\n\nPerformance Evaluation\nAs with other filters, the performance is quantified using PSNR by comparing the filtered image to the original, noise-free ground truth.\n\nExpected Outcome: The Nagao filter should yield a significantly higher PSNR than the mean filter when applied to noisy images containing sharp edges. It effectively smooths noise in uniform areas while protecting the high-frequency information that defines the edges.\nComputational Cost: The Nagao filter is computationally more expensive than the mean or median filters because it requires calculating multiple variances and means for every single pixel in the image. This trade-off between performance and computational cost is a common theme in image processing.\n"},"Labs/TP4---Mathematical-Morphology":{"slug":"Labs/TP4---Mathematical-Morphology","filePath":"Labs/TP4 - Mathematical Morphology.md","title":"TP4 - Mathematical Morphology","links":[],"tags":[],"content":"Lab 4: Mathematical Morphology\nObjective: To understand and apply fundamental morphological operations (Erosion and Dilation) to binary images for noise removal, object separation, and feature extraction.\n1. Core Concepts\nMathematical Morphology is a theory and technique for the analysis and processing of geometrical structures. It operates on images based on shape, using a small template called a Structuring Element (SE).\n\nStructuring Element (SE): This is the morphological equivalent of a kernel. It is a small binary mask (e.g., a 3x3 square, a cross, or a disk) that probes the image. The shape and size of the SE determine the effect of the operation.\nBinary Images: Morphological operations are most easily understood in the context of binary images, which contain only black (0) and white (255) pixels, representing background and foreground objects, respectively.\n\n2. Fundamental Operations\nErosion\n\nMechanism: Erosion “shrinks” or “thins” the foreground objects in an image. An output pixel is set to white (foreground) only if the entire structuring element fits completely inside the foreground region when its center is placed on that pixel. If any part of the SE covers a background pixel, the output pixel is set to black (background).\nApplications:\n\nNoise Removal: It effectively eliminates small, isolated white noise specks (salt noise), as the SE will not fit entirely within them.\nObject Separation: It can break thin connections between two objects that are touching.\n\n\n\nDilation\n\nMechanism: Dilation “expands” or “thickens” the foreground objects. An output pixel is set to white if at least one pixel of the structuring element covers a foreground pixel in the input image when the SE is centered on that pixel.\nApplications:\n\nNoise Removal: It fills in small holes or gaps within an object (pepper noise).\nObject Joining: It can connect parts of an object that are broken or separated by a small gap.\n\n\n\n3. Compound Operations\nErosion and Dilation are often used in sequence to perform more complex tasks.\nOpening\n\nDefinition: An erosion followed by a dilation, using the same structuring element for both.\n\nOpening(Image) = Dilate(Erode(Image))\n\n\nEffect: The erosion first removes small noise specks. The subsequent dilation then restores the size of the remaining objects without re-introducing the noise that was just removed.\nInsight: Opening is used to remove small objects and smooth object contours from the outside without significantly changing the size of the main objects. It’s like “opening up” small gaps and removing thin protrusions.\n\nClosing\n\nDefinition: A dilation followed by an erosion, using the same structuring element for both.\n\nClosing(Image) = Erode(Dilate(Image))\n\n\nEffect: The dilation first fills small holes inside objects. The subsequent erosion then shrinks the objects back to their original size without re-opening the holes that were just filled.\nInsight: Closing is used to fill small holes and connect small gaps in objects. It smooths contours from the inside.\n\n4. Application: Counting Blood Cells\nThe lab exercise involves using these morphological operations to facilitate the counting of red blood cells.\n\nProblem: The original binary image may have cells that are touching or have small noisy artifacts.\nSolution Strategy:\n\nNoise Removal: An Opening operation can be used to eliminate any small, non-cell artifacts.\nObject Separation: An Erosion operation can be applied to shrink the cells. If two cells are connected by a thin bridge, the erosion will likely break this connection, separating them into two distinct objects.\nObject Restoration: A subsequent Dilation can partially restore the size of the separated cells for better visualization.\n\n\nAfter these steps, standard algorithms for counting connected components (blobs) can be applied to get an accurate cell count.\n"},"index":{"slug":"index","filePath":"index.md","title":"index","links":[],"tags":[],"content":"Mersel Was Here"}}